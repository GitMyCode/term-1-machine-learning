{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TensorFlow ReLUs\n",
    "TensorFlow provides the ReLU function as tf.nn.relu(), as shown below.\n",
    "\n",
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "The above code applies the tf.nn.relu() function to the hidden_layer, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.zeros(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer) \n",
    "\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen the linear function tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer']) before, also known as xw + b. Combining linear functions together using a ReLU will give you a two layer network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'] )\n",
    "\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "# output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "This is the same optimization technique used in the Intro to TensorFLow la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session\n",
    "The MNIST library in TensorFlow provides the ability to receive the dataset in batches. Calling the mnist.train.next_batch() function returns a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:01<00:00, 219.50batches/s]\n",
      "Epoch 2/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 203.48batches/s]\n",
      "Epoch 3/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 193.68batches/s]\n",
      "Epoch 4/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 193.43batches/s]\n",
      "Epoch 5/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 189.04batches/s]\n",
      "Epoch 6/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 193.95batches/s]\n",
      "Epoch 7/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 181.47batches/s]\n",
      "Epoch 8/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 182.76batches/s]\n",
      "Epoch 9/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 164.35batches/s]\n",
      "Epoch 10/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 184.61batches/s]\n",
      "Epoch 11/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 179.71batches/s]\n",
      "Epoch 12/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 187.93batches/s]\n",
      "Epoch 13/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 185.83batches/s]\n",
      "Epoch 14/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 188.80batches/s]\n",
      "Epoch 15/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 183.63batches/s]\n",
      "Epoch 16/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 186.24batches/s]\n",
      "Epoch 17/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 181.01batches/s]\n",
      "Epoch 18/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 176.14batches/s]\n",
      "Epoch 19/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 178.90batches/s]\n",
      "Epoch 20/20: 100%|████████████████████████████████████████████████████████████████████████████████████████| 429/429 [00:02<00:00, 189.59batches/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        batches_pbar = tqdm(range(total_batch), desc='Epoch {0}/{1}'.format(epoch+1, training_epochs), unit='batches')\n",
    "        \n",
    "        for i in batches_pbar:\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Restore TensorFlow Models\n",
    "Training a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again!\n",
    "\n",
    "Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system.\n",
    "\n",
    "#### Saving Variables\n",
    "Let's start with a simple example of saving weights and bias Tensors. For the first example you'll just save two variables. Later examples will save all the weights in a practical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[-0.85024285  1.56873643 -0.23223434]\n",
      " [-1.62584257 -0.9287324   0.29016298]]\n",
      "Bias:\n",
      "[ 0.00830418 -0.48626173 -0.69528824]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([2,3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Show the values of weights and bias\n",
    "    print('Weights:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "    \n",
    "    saver.save(sess, save_file)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:\n",
      "[[-0.04061302  0.54202622 -0.86118251]\n",
      " [ 0.26749384 -0.62197828 -0.74575353]]\n",
      "Bias:\n",
      "[-0.400325    0.56635517 -0.53533804]\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weight:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "\n",
    "mnist = input_data.read_data_sets('.', one_hot= True)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.truncated_normal([n_classes]))\n",
    "# xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train de model and save de weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Validation accuracy: 0.058400001376867294\n",
      "Epoch 10  - Validation accuracy: 0.2919999957084656\n",
      "Epoch 20  - Validation accuracy: 0.4399999976158142\n",
      "Epoch 30  - Validation accuracy: 0.5343999862670898\n",
      "Epoch 40  - Validation accuracy: 0.6021999716758728\n",
      "Epoch 50  - Validation accuracy: 0.644599974155426\n",
      "Epoch 60  - Validation accuracy: 0.6782000064849854\n",
      "Epoch 70  - Validation accuracy: 0.7006000280380249\n",
      "Epoch 80  - Validation accuracy: 0.72079998254776\n",
      "Epoch 90  - Validation accuracy: 0.7350000143051147\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "            \n",
    "        #print status every 10 batch\n",
    "        if epoch %10 ==0 :\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict= \n",
    "                                      {features: mnist.validation.images,\n",
    "                                       labels: mnist.validation.labels })\n",
    "            print('Epoch {:<3} - Validation accuracy: {}'.format(epoch, valid_accuracy))\n",
    "    saver.save(sess, save_file)\n",
    "    print(\"Finish training\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load a trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7461000084877014\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    \n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict = {features: mnist.test.images, labels: mnist.test.labels}\n",
    "    )\n",
    "    \n",
    "    print(\"Test accuracy: {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Dropdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "\n",
    "\n",
    "# TODO: Print logits from a session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.55999947  16.        ]\n",
      " [  0.           0.        ]\n",
      " [ 38.4600029   71.69999695]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create Model\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer) \n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(output, feed_dict={keep_prob : 0.5}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
