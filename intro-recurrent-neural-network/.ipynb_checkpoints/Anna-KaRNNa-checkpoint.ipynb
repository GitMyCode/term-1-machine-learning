{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 19, 36, 56, 77, 82, 62,  8, 10, 30, 30, 30,  9, 36, 56, 56, 69,\n",
       "        8, 61, 36, 76, 38, 49, 38, 82, 79,  8, 36, 62, 82,  8, 36, 49, 49,\n",
       "        8, 36, 49, 38, 35, 82,  3,  8, 82, 44, 82, 62, 69,  8,  0, 47, 19,\n",
       "       36, 56, 56, 69,  8, 61, 36, 76, 38, 49, 69,  8, 38, 79,  8,  0, 47,\n",
       "       19, 36, 56, 56, 69,  8, 38, 47,  8, 38, 77, 79,  8, 31, 66, 47, 30,\n",
       "       66, 36, 69, 57, 30, 30, 75, 44, 82, 62, 69, 77, 19, 38, 47])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23, 19, 36, 56, 77, 82, 62,  8, 10, 30, 30, 30,  9, 36, 56, 56, 69,\n",
       "         8, 61, 36, 76, 38, 49, 38, 82, 79,  8, 36, 62, 82,  8, 36, 49, 49,\n",
       "         8, 36, 49, 38, 35, 82,  3,  8, 82, 44, 82, 62, 69,  8,  0, 47],\n",
       "       [ 8, 36, 76,  8, 47, 31, 77,  8, 42, 31, 38, 47, 42,  8, 77, 31,  8,\n",
       "        79, 77, 36, 69, 70, 21,  8, 36, 47, 79, 66, 82, 62, 82, 11,  8, 26,\n",
       "        47, 47, 36, 70,  8, 79, 76, 38, 49, 38, 47, 42, 70,  8, 51,  0],\n",
       "       [44, 38, 47, 57, 30, 30, 21, 32, 82, 79, 70,  8, 38, 77, 43, 79,  8,\n",
       "        79, 82, 77, 77, 49, 82, 11, 57,  8, 74, 19, 82,  8, 56, 62, 38, 67,\n",
       "        82,  8, 38, 79,  8, 76, 36, 42, 47, 38, 61, 38, 67, 82, 47, 77],\n",
       "       [47,  8, 11,  0, 62, 38, 47, 42,  8, 19, 38, 79,  8, 67, 31, 47, 44,\n",
       "        82, 62, 79, 36, 77, 38, 31, 47,  8, 66, 38, 77, 19,  8, 19, 38, 79,\n",
       "        30, 51, 62, 31, 77, 19, 82, 62,  8, 66, 36, 79,  8, 77, 19, 38],\n",
       "       [ 8, 38, 77,  8, 38, 79, 70,  8, 79, 38, 62, 65, 21,  8, 79, 36, 38,\n",
       "        11,  8, 77, 19, 82,  8, 31, 49, 11,  8, 76, 36, 47, 70,  8, 42, 82,\n",
       "        77, 77, 38, 47, 42,  8,  0, 56, 70,  8, 36, 47, 11, 30, 67, 62],\n",
       "       [ 8, 81, 77,  8, 66, 36, 79, 30, 31, 47, 49, 69,  8, 66, 19, 82, 47,\n",
       "         8, 77, 19, 82,  8, 79, 36, 76, 82,  8, 82, 44, 82, 47, 38, 47, 42,\n",
       "         8, 19, 82,  8, 67, 36, 76, 82,  8, 77, 31,  8, 77, 19, 82, 38],\n",
       "       [19, 82, 47,  8, 67, 31, 76, 82,  8, 61, 31, 62,  8, 76, 82, 70, 21,\n",
       "         8, 79, 19, 82,  8, 79, 36, 38, 11, 70,  8, 36, 47, 11,  8, 66, 82,\n",
       "        47, 77,  8, 51, 36, 67, 35,  8, 38, 47, 77, 31,  8, 77, 19, 82],\n",
       "       [ 3,  8, 51,  0, 77,  8, 47, 31, 66,  8, 79, 19, 82,  8, 66, 31,  0,\n",
       "        49, 11,  8, 62, 82, 36, 11, 38, 49, 69,  8, 19, 36, 44, 82,  8, 79,\n",
       "        36, 67, 62, 38, 61, 38, 67, 82, 11, 70,  8, 47, 31, 77,  8, 76],\n",
       "       [77,  8, 38, 79, 47, 43, 77, 57,  8, 74, 19, 82, 69, 43, 62, 82,  8,\n",
       "        56, 62, 31, 56, 62, 38, 82, 77, 31, 62, 79,  8, 31, 61,  8, 36,  8,\n",
       "        79, 31, 62, 77, 70, 30, 51,  0, 77,  8, 66, 82, 43, 62, 82,  8],\n",
       "       [ 8, 79, 36, 38, 11,  8, 77, 31,  8, 19, 82, 62, 79, 82, 49, 61, 70,\n",
       "         8, 36, 47, 11,  8, 51, 82, 42, 36, 47,  8, 36, 42, 36, 38, 47,  8,\n",
       "        61, 62, 31, 76,  8, 77, 19, 82,  8, 51, 82, 42, 38, 47, 47, 38]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2  Iteration 1/356 Training loss: 4.4178 5.1824 sec/batch\n",
      "Epoch 1/2  Iteration 2/356 Training loss: 4.3738 3.7585 sec/batch\n",
      "Epoch 1/2  Iteration 3/356 Training loss: 4.1856 3.9515 sec/batch\n",
      "Epoch 1/2  Iteration 4/356 Training loss: 4.5017 4.0192 sec/batch\n",
      "Epoch 1/2  Iteration 5/356 Training loss: 4.4550 3.8914 sec/batch\n",
      "Epoch 1/2  Iteration 6/356 Training loss: 4.3579 3.8888 sec/batch\n",
      "Epoch 1/2  Iteration 7/356 Training loss: 4.2737 4.0017 sec/batch\n",
      "Epoch 1/2  Iteration 8/356 Training loss: 4.1983 4.0237 sec/batch\n",
      "Epoch 1/2  Iteration 9/356 Training loss: 4.1257 3.9342 sec/batch\n",
      "Epoch 1/2  Iteration 10/356 Training loss: 4.0630 3.9144 sec/batch\n",
      "Epoch 1/2  Iteration 11/356 Training loss: 4.0052 3.9691 sec/batch\n",
      "Epoch 1/2  Iteration 12/356 Training loss: 3.9544 4.5135 sec/batch\n",
      "Epoch 1/2  Iteration 13/356 Training loss: 3.9085 4.2724 sec/batch\n",
      "Epoch 1/2  Iteration 14/356 Training loss: 3.8705 4.1972 sec/batch\n",
      "Epoch 1/2  Iteration 15/356 Training loss: 3.8365 4.1772 sec/batch\n",
      "Epoch 1/2  Iteration 16/356 Training loss: 3.8052 3.9761 sec/batch\n",
      "Epoch 1/2  Iteration 17/356 Training loss: 3.7762 3.9666 sec/batch\n",
      "Epoch 1/2  Iteration 18/356 Training loss: 3.7518 4.1703 sec/batch\n",
      "Epoch 1/2  Iteration 19/356 Training loss: 3.7286 4.4560 sec/batch\n",
      "Epoch 1/2  Iteration 20/356 Training loss: 3.7054 4.1967 sec/batch\n",
      "Epoch 1/2  Iteration 21/356 Training loss: 3.6853 4.1441 sec/batch\n",
      "Epoch 1/2  Iteration 22/356 Training loss: 3.6670 4.1544 sec/batch\n",
      "Epoch 1/2  Iteration 23/356 Training loss: 3.6493 4.0357 sec/batch\n",
      "Epoch 1/2  Iteration 24/356 Training loss: 3.6330 3.9340 sec/batch\n",
      "Epoch 1/2  Iteration 25/356 Training loss: 3.6175 4.3557 sec/batch\n",
      "Epoch 1/2  Iteration 26/356 Training loss: 3.6038 4.2513 sec/batch\n",
      "Epoch 1/2  Iteration 27/356 Training loss: 3.5909 4.2143 sec/batch\n",
      "Epoch 1/2  Iteration 28/356 Training loss: 3.5780 4.0504 sec/batch\n",
      "Epoch 1/2  Iteration 29/356 Training loss: 3.5661 3.9856 sec/batch\n",
      "Epoch 1/2  Iteration 30/356 Training loss: 3.5550 4.1440 sec/batch\n",
      "Epoch 1/2  Iteration 31/356 Training loss: 3.5454 4.3591 sec/batch\n",
      "Epoch 1/2  Iteration 32/356 Training loss: 3.5354 4.2599 sec/batch\n",
      "Epoch 1/2  Iteration 33/356 Training loss: 3.5254 4.2067 sec/batch\n",
      "Epoch 1/2  Iteration 34/356 Training loss: 3.5167 4.1331 sec/batch\n",
      "Epoch 1/2  Iteration 35/356 Training loss: 3.5076 3.9280 sec/batch\n",
      "Epoch 1/2  Iteration 36/356 Training loss: 3.4994 4.2608 sec/batch\n",
      "Epoch 1/2  Iteration 37/356 Training loss: 3.4911 4.4158 sec/batch\n",
      "Epoch 1/2  Iteration 38/356 Training loss: 3.4831 4.2825 sec/batch\n",
      "Epoch 1/2  Iteration 39/356 Training loss: 3.4755 4.1636 sec/batch\n",
      "Epoch 1/2  Iteration 40/356 Training loss: 3.4684 4.0573 sec/batch\n",
      "Epoch 1/2  Iteration 41/356 Training loss: 3.4615 4.0549 sec/batch\n",
      "Epoch 1/2  Iteration 42/356 Training loss: 3.4548 4.0849 sec/batch\n",
      "Epoch 1/2  Iteration 43/356 Training loss: 3.4483 3.9896 sec/batch\n",
      "Epoch 1/2  Iteration 44/356 Training loss: 3.4420 4.4052 sec/batch\n",
      "Epoch 1/2  Iteration 45/356 Training loss: 3.4358 4.3651 sec/batch\n",
      "Epoch 1/2  Iteration 46/356 Training loss: 3.4301 4.2840 sec/batch\n",
      "Epoch 1/2  Iteration 47/356 Training loss: 3.4249 4.1120 sec/batch\n",
      "Epoch 1/2  Iteration 48/356 Training loss: 3.4198 4.1079 sec/batch\n",
      "Epoch 1/2  Iteration 49/356 Training loss: 3.4150 4.2294 sec/batch\n",
      "Epoch 1/2  Iteration 50/356 Training loss: 3.4101 4.3812 sec/batch\n",
      "Epoch 1/2  Iteration 51/356 Training loss: 3.4054 4.2162 sec/batch\n",
      "Epoch 1/2  Iteration 52/356 Training loss: 3.4008 4.2694 sec/batch\n",
      "Epoch 1/2  Iteration 53/356 Training loss: 3.3964 4.1791 sec/batch\n",
      "Epoch 1/2  Iteration 54/356 Training loss: 3.3920 3.9461 sec/batch\n",
      "Epoch 1/2  Iteration 55/356 Training loss: 3.3879 3.8753 sec/batch\n",
      "Epoch 1/2  Iteration 56/356 Training loss: 3.3835 3.9801 sec/batch\n",
      "Epoch 1/2  Iteration 57/356 Training loss: 3.3795 4.0189 sec/batch\n",
      "Epoch 1/2  Iteration 58/356 Training loss: 3.3756 4.2628 sec/batch\n",
      "Epoch 1/2  Iteration 59/356 Training loss: 3.3717 4.2974 sec/batch\n",
      "Epoch 1/2  Iteration 60/356 Training loss: 3.3683 4.2212 sec/batch\n",
      "Epoch 1/2  Iteration 61/356 Training loss: 3.3647 4.1340 sec/batch\n",
      "Epoch 1/2  Iteration 62/356 Training loss: 3.3617 4.0428 sec/batch\n",
      "Epoch 1/2  Iteration 63/356 Training loss: 3.3587 3.9495 sec/batch\n",
      "Epoch 1/2  Iteration 64/356 Training loss: 3.3552 4.0237 sec/batch\n",
      "Epoch 1/2  Iteration 65/356 Training loss: 3.3519 3.8648 sec/batch\n",
      "Epoch 1/2  Iteration 66/356 Training loss: 3.3490 4.3672 sec/batch\n",
      "Epoch 1/2  Iteration 67/356 Training loss: 3.3462 4.4012 sec/batch\n",
      "Epoch 1/2  Iteration 68/356 Training loss: 3.3427 4.3686 sec/batch\n",
      "Epoch 1/2  Iteration 69/356 Training loss: 3.3396 4.1031 sec/batch\n",
      "Epoch 1/2  Iteration 70/356 Training loss: 3.3370 3.9516 sec/batch\n",
      "Epoch 1/2  Iteration 71/356 Training loss: 3.3341 3.9385 sec/batch\n",
      "Epoch 1/2  Iteration 72/356 Training loss: 3.3316 4.0112 sec/batch\n",
      "Epoch 1/2  Iteration 73/356 Training loss: 3.3290 5.4797 sec/batch\n",
      "Epoch 1/2  Iteration 74/356 Training loss: 3.3263 5.1176 sec/batch\n",
      "Epoch 1/2  Iteration 75/356 Training loss: 3.3240 4.7276 sec/batch\n",
      "Epoch 1/2  Iteration 76/356 Training loss: 3.3216 4.4670 sec/batch\n",
      "Epoch 1/2  Iteration 77/356 Training loss: 3.3193 4.3631 sec/batch\n",
      "Epoch 1/2  Iteration 78/356 Training loss: 3.3169 4.3606 sec/batch\n",
      "Epoch 1/2  Iteration 79/356 Training loss: 3.3145 4.3100 sec/batch\n",
      "Epoch 1/2  Iteration 80/356 Training loss: 3.3120 3.9831 sec/batch\n",
      "Epoch 1/2  Iteration 81/356 Training loss: 3.3096 3.9054 sec/batch\n",
      "Epoch 1/2  Iteration 82/356 Training loss: 3.3075 3.9495 sec/batch\n",
      "Epoch 1/2  Iteration 83/356 Training loss: 3.3054 3.9084 sec/batch\n",
      "Epoch 1/2  Iteration 84/356 Training loss: 3.3032 3.8883 sec/batch\n",
      "Epoch 1/2  Iteration 85/356 Training loss: 3.3008 4.3456 sec/batch\n",
      "Epoch 1/2  Iteration 86/356 Training loss: 3.2987 4.2754 sec/batch\n",
      "Epoch 1/2  Iteration 87/356 Training loss: 3.2965 4.1857 sec/batch\n",
      "Epoch 1/2  Iteration 88/356 Training loss: 3.2943 4.0037 sec/batch\n",
      "Epoch 1/2  Iteration 89/356 Training loss: 3.2930 3.9796 sec/batch\n",
      "Epoch 1/2  Iteration 90/356 Training loss: 3.2937 3.9410 sec/batch\n",
      "Epoch 1/2  Iteration 91/356 Training loss: 3.2946 4.2463 sec/batch\n",
      "Epoch 1/2  Iteration 92/356 Training loss: 3.2953 4.2373 sec/batch\n",
      "Epoch 1/2  Iteration 93/356 Training loss: 3.2958 4.1450 sec/batch\n",
      "Epoch 1/2  Iteration 94/356 Training loss: 3.2960 4.0844 sec/batch\n",
      "Epoch 1/2  Iteration 95/356 Training loss: 3.2958 3.9069 sec/batch\n",
      "Epoch 1/2  Iteration 96/356 Training loss: 3.2954 3.8202 sec/batch\n",
      "Epoch 1/2  Iteration 97/356 Training loss: 3.2952 3.8593 sec/batch\n",
      "Epoch 1/2  Iteration 98/356 Training loss: 3.2946 4.2308 sec/batch\n",
      "Epoch 1/2  Iteration 99/356 Training loss: 3.2940 4.1420 sec/batch\n",
      "Epoch 1/2  Iteration 100/356 Training loss: 3.2931 4.0628 sec/batch\n",
      "Epoch 1/2  Iteration 101/356 Training loss: 3.2922 3.9726 sec/batch\n",
      "Epoch 1/2  Iteration 102/356 Training loss: 3.2911 3.9525 sec/batch\n",
      "Epoch 1/2  Iteration 103/356 Training loss: 3.2900 3.8182 sec/batch\n",
      "Epoch 1/2  Iteration 104/356 Training loss: 3.2886 3.9726 sec/batch\n",
      "Epoch 1/2  Iteration 105/356 Training loss: 3.2872 4.2413 sec/batch\n",
      "Epoch 1/2  Iteration 106/356 Training loss: 3.2859 4.1079 sec/batch\n",
      "Epoch 1/2  Iteration 107/356 Training loss: 3.2844 3.9555 sec/batch\n",
      "Epoch 1/2  Iteration 108/356 Training loss: 3.2829 3.9826 sec/batch\n",
      "Epoch 1/2  Iteration 109/356 Training loss: 3.2815 3.9244 sec/batch\n",
      "Epoch 1/2  Iteration 110/356 Training loss: 3.2799 3.8151 sec/batch\n",
      "Epoch 1/2  Iteration 111/356 Training loss: 3.2786 3.7720 sec/batch\n",
      "Epoch 1/2  Iteration 112/356 Training loss: 3.2772 3.8021 sec/batch\n",
      "Epoch 1/2  Iteration 113/356 Training loss: 3.2758 4.3576 sec/batch\n",
      "Epoch 1/2  Iteration 114/356 Training loss: 3.2743 4.1009 sec/batch\n",
      "Epoch 1/2  Iteration 115/356 Training loss: 3.2729 4.0528 sec/batch\n",
      "Epoch 1/2  Iteration 116/356 Training loss: 3.2715 3.9595 sec/batch\n",
      "Epoch 1/2  Iteration 117/356 Training loss: 3.2700 3.9515 sec/batch\n",
      "Epoch 1/2  Iteration 118/356 Training loss: 3.2687 3.8071 sec/batch\n",
      "Epoch 1/2  Iteration 119/356 Training loss: 3.2675 3.7279 sec/batch\n",
      "Epoch 1/2  Iteration 120/356 Training loss: 3.2660 3.8212 sec/batch\n",
      "Epoch 1/2  Iteration 121/356 Training loss: 3.2649 4.2603 sec/batch\n",
      "Epoch 1/2  Iteration 122/356 Training loss: 3.2635 4.1039 sec/batch\n",
      "Epoch 1/2  Iteration 123/356 Training loss: 3.2622 4.1159 sec/batch\n",
      "Epoch 1/2  Iteration 124/356 Training loss: 3.2610 4.0407 sec/batch\n",
      "Epoch 1/2  Iteration 125/356 Training loss: 3.2595 3.8623 sec/batch\n",
      "Epoch 1/2  Iteration 126/356 Training loss: 3.2579 3.8182 sec/batch\n",
      "Epoch 1/2  Iteration 127/356 Training loss: 3.2566 3.8362 sec/batch\n",
      "Epoch 1/2  Iteration 128/356 Training loss: 3.2552 3.8954 sec/batch\n",
      "Epoch 1/2  Iteration 129/356 Training loss: 3.2537 3.7630 sec/batch\n",
      "Epoch 1/2  Iteration 130/356 Training loss: 3.2524 4.2682 sec/batch\n",
      "Epoch 1/2  Iteration 131/356 Training loss: 3.2513 4.4659 sec/batch\n",
      "Epoch 1/2  Iteration 132/356 Training loss: 3.2499 4.4012 sec/batch\n",
      "Epoch 1/2  Iteration 133/356 Training loss: 3.2486 4.2177 sec/batch\n",
      "Epoch 1/2  Iteration 134/356 Training loss: 3.2472 4.1435 sec/batch\n",
      "Epoch 1/2  Iteration 135/356 Training loss: 3.2457 4.1706 sec/batch\n",
      "Epoch 1/2  Iteration 136/356 Training loss: 3.2442 4.1370 sec/batch\n",
      "Epoch 1/2  Iteration 137/356 Training loss: 3.2428 3.9761 sec/batch\n",
      "Epoch 1/2  Iteration 138/356 Training loss: 3.2413 4.4854 sec/batch\n",
      "Epoch 1/2  Iteration 139/356 Training loss: 3.2400 4.4253 sec/batch\n",
      "Epoch 1/2  Iteration 140/356 Training loss: 3.2386 4.2834 sec/batch\n",
      "Epoch 1/2  Iteration 141/356 Training loss: 3.2372 4.2363 sec/batch\n",
      "Epoch 1/2  Iteration 142/356 Training loss: 3.2355 4.0814 sec/batch\n",
      "Epoch 1/2  Iteration 143/356 Training loss: 3.2340 4.0082 sec/batch\n",
      "Epoch 1/2  Iteration 144/356 Training loss: 3.2323 3.9154 sec/batch\n",
      "Epoch 1/2  Iteration 145/356 Training loss: 3.2307 3.9175 sec/batch\n",
      "Epoch 1/2  Iteration 146/356 Training loss: 3.2291 4.3737 sec/batch\n",
      "Epoch 1/2  Iteration 147/356 Training loss: 3.2276 4.3797 sec/batch\n",
      "Epoch 1/2  Iteration 148/356 Training loss: 3.2260 4.1851 sec/batch\n",
      "Epoch 1/2  Iteration 149/356 Training loss: 3.2242 4.0834 sec/batch\n",
      "Epoch 1/2  Iteration 150/356 Training loss: 3.2225 4.0262 sec/batch\n",
      "Epoch 1/2  Iteration 151/356 Training loss: 3.2209 4.0036 sec/batch\n",
      "Epoch 1/2  Iteration 152/356 Training loss: 3.2194 3.9109 sec/batch\n",
      "Epoch 1/2  Iteration 153/356 Training loss: 3.2175 3.9079 sec/batch\n",
      "Epoch 1/2  Iteration 154/356 Training loss: 3.2157 4.3055 sec/batch\n",
      "Epoch 1/2  Iteration 155/356 Training loss: 3.2139 4.1932 sec/batch\n",
      "Epoch 1/2  Iteration 156/356 Training loss: 3.2119 4.0753 sec/batch\n",
      "Epoch 1/2  Iteration 157/356 Training loss: 3.2100 4.0568 sec/batch\n",
      "Epoch 1/2  Iteration 158/356 Training loss: 3.2080 4.1466 sec/batch\n",
      "Epoch 1/2  Iteration 159/356 Training loss: 3.2059 4.1200 sec/batch\n",
      "Epoch 1/2  Iteration 160/356 Training loss: 3.2039 3.7189 sec/batch\n",
      "Epoch 1/2  Iteration 161/356 Training loss: 3.2018 4.1285 sec/batch\n",
      "Epoch 1/2  Iteration 162/356 Training loss: 3.1996 4.0914 sec/batch\n",
      "Epoch 1/2  Iteration 163/356 Training loss: 3.1974 3.8778 sec/batch\n",
      "Epoch 1/2  Iteration 164/356 Training loss: 3.1954 3.7811 sec/batch\n",
      "Epoch 1/2  Iteration 165/356 Training loss: 3.1932 4.2298 sec/batch\n",
      "Epoch 1/2  Iteration 166/356 Training loss: 3.1910 4.2108 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4ea0fa88971b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     34\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 35\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
